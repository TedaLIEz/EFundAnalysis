---
description: This rule is helpful for building an AI agent in the project, you should refer to this rule when you write AI agent like RAG, ReAct agents, etc.
globs: core/llm/**/*.py
alwaysApply: false
---
You are an expert in Python, building AI agents by leveraging llamaindex.


Key Principles

- Write concise, technical responses with accurate Python examples.
- Use functional, declarative programming; avoid classes where possible.
- Prefer iteration and modularization over code duplication.
- Use descriptive variable names with auxiliary verbs (e.g., is_active, has_permission).
- Use lowercase with underscores for directories and files (e.g., routers/user_routes.py).
- Favor named exports for routes and utility functions.
- Use the Receive an Object, Return an Object (RORO) pattern.
- Manage runtime within python virtual envrionment, this has been already included in the @venv folder.
- Manage dependencies with uv, there is one [pyproject.toml] in the codebase.
- Always run python command under virtual environment by activating virtual environment with `source venv/bin/activate`

LLM Chat Functions

- **Always prioritize streaming for LLM chat functions**: When implementing chat functionality with LLMs, streaming should be the primary approach.
- **Provide both sync and async streaming methods**: Implement both synchronous (`stream_chat`) and asynchronous (`astream_chat`) streaming methods for maximum flexibility.
- **Streaming benefits**: Streaming provides better user experience (progressive response display), lower perceived latency, and more efficient memory usage for long responses.
- **Implementation pattern**:
  - Use generators (`Generator[str, None]`) for synchronous streaming
  - Use async generators (`AsyncGenerator[str, None]`) for asynchronous streaming
  - Yield chunks as they arrive from the LLM or workflow
  - Handle errors gracefully within the streaming loop
- **Non-streaming as fallback**: Non-streaming methods (`chat`) can be provided as convenience methods, but streaming should be the default implementation.
- **Memory management**: Ensure conversation memory is updated appropriately after streaming completes (either incrementally or at the end).


Folder structure
- The [agent.py](mdc:src/core/agent/agent.py) contains the class object to set up an AI agent.
- The [siliconflow.py](mdc:src/core/llm/siliconflow.py) provides the model via SiliconFlow.
- The [rag.py](mdc:src/core/rag/rag.py) provides a template of building a RAG.

Error Handling and Validation

- Prioritize error handling and edge cases:
  - Handle errors and edge cases at the beginning of functions.
  - Use early returns for error conditions to avoid deeply nested if statements.
  - Place the happy path last in the function for improved readability.
  - Avoid unnecessary else statements; use the if-return pattern instead.
  - Use guard clauses to handle preconditions and invalid states early.
  - Implement proper error logging and user-friendly error messages.
  - Use custom error types or error factories for consistent error handling.
  -

Dependencies

- Pydantic v2
- Llamaindex 0.12.24

Refer to Llamaindex documentation for Agents, RAG, Embeddings, Memory, Query Engines and Multi-Model for best practices.
