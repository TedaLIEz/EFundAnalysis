---
description: This rule is helpful for building an AI agent in the project, you should refer to this rule when you write AI agent like RAG, ReAct agents, etc.
globs: core/llm/**/*.py
alwaysApply: false
---

# Python LLM Rule

You are an expert in Python, building AI agents by leveraging llamaindex.

## Key Principles

- Write concise, technical responses with accurate Python examples.
- Use functional, declarative programming; avoid classes where possible.
- Prefer iteration and modularization over code duplication.
- Use descriptive variable names with auxiliary verbs (e.g., is_active, has_permission).
- Use lowercase with underscores for directories and files (e.g., routers/user_routes.py).
- Favor named exports for routes and utility functions.
- Use the Receive an Object, Return an Object (RORO) pattern.
- Manage runtime within python virtual environment using `uv` (see overall project rules)
- Always use `uv run` prefix for Python commands (e.g., `uv run python app.py`)
- Manage dependencies with `uv sync` and `pyproject.toml`

## LLM Chat Functions

- **Always prioritize streaming for LLM chat functions**: When implementing chat functionality with LLMs, streaming should be the primary approach.
- **Provide both sync and async streaming methods**: Implement both synchronous (`stream_chat`) and asynchronous (`astream_chat`) streaming methods for maximum flexibility.
- **Streaming benefits**: Streaming provides better user experience (progressive response display), lower perceived latency, and more efficient memory usage for long responses.
- **Implementation pattern**:
  - Use generators (`Generator[str, None]`) for synchronous streaming
  - Use async generators (`AsyncGenerator[str, None]`) for asynchronous streaming
  - Yield chunks as they arrive from the LLM or workflow
  - Handle errors gracefully within the streaming loop
- **Non-streaming as fallback**: Non-streaming methods (`chat`) can be provided as convenience methods, but streaming should be the default implementation.
- **Memory management**: Ensure conversation memory is updated appropriately after streaming completes (either incrementally or at the end).

## Folder Structure

The LLM module is organized into the following components:

### Core Components

- **`core/llm/agent/agent.py`**: ReActAgent implementation with function calling capabilities
  - Uses `ReActAgent` from LlamaIndex
  - Supports tool registration and function calling
  - Provides `run()` method for synchronous execution
  - Provides `get_registered_functions()` method that returns `FunctionDescription` objects
  - Provides `chat_history` property to access conversation history
  - Supports optional `memory` parameter for conversation history management
  - Supports optional `tools` parameter (list of callable functions)
  - Supports optional `verbose` parameter (defaults to True)
  - Handles max iteration errors gracefully with user-friendly error messages

- **`core/llm/chat/chatbot.py`**: Basic chatbot with conversation memory
  - Uses `SimpleChatEngine` from LlamaIndex
  - Implements both sync (`stream_chat`) and async (`astream_chat`) streaming
  - Provides non-streaming `chat()` method as fallback
  - Uses `Memory.from_defaults()` for conversation history (not ChatMemoryBuffer)
  - Supports system prompts
  - Provides `reset()` method to clear conversation memory
  - Provides `get_chat_history()` method to retrieve chat history as list of dicts

- **`core/llm/rag/rag.py`**: RAG (Retrieval-Augmented Generation) implementation
  - Uses `VectorStoreIndex` for document indexing
  - Uses `SimpleDirectoryReader` for loading documents from directories
  - Provides `load_document()` method to load and index documents
  - Provides async `query()` method with custom prompt templates
  - Requires both LLM (`FunctionCallingLLM`) and embedding model (`BaseEmbedding`)
  - Custom prompt template emphasizes concise answers and "I don't know" for unknown queries

### Model Management

- **`core/llm/model/factory.py`**: Factory pattern for creating LLM instances
  - `LLMFactory` class supports multiple providers (azure_openai, siliconflow)
  - Uses environment variable `LLM_PROVIDER` to determine default provider (defaults to "azure_openai")
  - Provides `create()` class method to create LLM instances
  - Provides `get_supported_providers()` class method to list available providers
  - Provides `register_provider()` class method for extensibility
  - Returns `FunctionCallingLLM` instances
  - Raises `ValueError` for unsupported providers

- **`core/llm/model/aopenai.py`**: Azure OpenAI provider implementation
  - Exports `create_llm()` function (used internally by factory)
  - Creates `AzureOpenAI` instances from llama_index.llms.azure_openai
  - Uses environment variables: `AZURE_OPENAI_KEY`, `AZURE_OPENAI_ENDPOINT`, `AZURE_OPENAI_VERSION`, `AZURE_OPENAI_MODEL_NAME`, `AZURE_OPENAI_DEPLOYMENT`
  - Returns `FunctionCallingLLM` instances

- **`core/llm/model/siliconflow.py`**: SiliconFlow provider implementation
  - Exports `create_llm()` function (used internally by factory)
  - Creates `SiliconFlow` instances from llama_index.llms.siliconflow
  - Uses environment variables: `ENDPOINT`, `API_KEY`, `LLM_MODEL`, `MAX_TOKENS`
  - Raises `ValueError` if required environment variables are missing
  - Sets temperature=0 by default
  - Returns `FunctionCallingLLM` instances

- **`core/llm/model/__init__.py`**: Exports `create_llm()`, `LLMFactory`, and `LLMProvider`
  - Maintains backward compatibility with `create_llm()` function

### Embedding Models

- **`core/llm/embedding/siliconflow.py`**: SiliconFlow embedding model
  - Exports `create_embedding_model()` function
  - Creates `SiliconFlowEmbedding` instances from llama_index.embeddings.siliconflow
  - Uses environment variables: `ENDPOINT`, `API_KEY`, `EMBEDDING_MODEL`
  - Raises `ValueError` if required environment variables are missing
  - Returns `BaseEmbedding` instances

### Memory Management

- **`core/llm/memory/memory.py`**: Advanced memory system using LlamaIndex's Memory framework
  - `LLMMemory` class wraps LlamaIndex's `Memory` class
  - Supports multiple memory types:
    - **Short-term memory**: Chat history with token limits (default: 3000 tokens)
    - **Static memory**: Fixed information that persists across conversations (uses `StaticMemoryBlock` with `TextBlock`)
    - **Fact extraction memory**: Automatically extracts and stores facts from conversations (uses `FactExtractionMemoryBlock`)
    - **Vector memory**: Semantic search over conversation history (uses `VectorMemoryBlock`)
  - Factory methods:
    - `from_defaults()`: Short-term memory only (token_limit parameter)
    - `with_static_memory()`: Adds static memory block (requires static_content string)
    - `with_fact_extraction()`: Adds fact extraction memory block (requires LLM)
    - `with_vector_memory()`: Adds vector memory block (requires embed_model and vector_store)
    - `with_all_memory_types()`: Combines all memory types (requires llm, embed_model, vector_store, optional static_content)
  - Returns `Memory` instance (implements `BaseMemory`) via `get_memory()`
  - Provides `reset()`, `get_all()`, `put()`, and `get()` methods for memory management

### Prompt Management

- **`core/llm/prompt/prompt_loader.py`**: Liquid template-based prompt loader
  - `PromptLoader` class loads prompts from `.liquid` template files
  - Uses `CachingFileSystemLoader` for efficient template loading
  - Supports template rendering with context variables
  - Default prompt directory: `./prompt`
  - Methods:
    - `load_prompt()`: Load prompt without variables
    - `load_prompt_with_context()`: Load prompt with variable substitution

### Tools

- **`core/llm/tool/`**: Utility functions for use as LlamaIndex function tools
  - `date_util.py`: Date utility functions
    - `get_current_date()`: Returns current date in ISO format (YYYY-MM-DD)
  - Functions can be registered with agents using `FunctionTool.from_defaults()`
  - Module exports `get_current_date` via `__init__.py`

## Error Handling and Validation

- Prioritize error handling and edge cases:
  - Handle errors and edge cases at the beginning of functions.
  - Use early returns for error conditions to avoid deeply nested if statements.
  - Place the happy path last in the function for improved readability.
  - Avoid unnecessary else statements; use the if-return pattern instead.
  - Use guard clauses to handle preconditions and invalid states early.
  - Implement proper error logging and user-friendly error messages.
  - Use custom error types or error factories for consistent error handling.

## Usage Patterns

### Creating an Agent

```python
from core.llm.agent import Agent
from core.llm.tool.date_util import get_current_date
from core.llm.memory import LLMMemory

# Create agent with default LLM and tools
agent = Agent(tools=[get_current_date])

# Create agent with custom memory
memory = LLMMemory.from_defaults(token_limit=5000)
agent = Agent(tools=[get_current_date], memory=memory.get_memory())

# Create agent with custom LLM and verbose mode
from core.llm.model import LLMFactory
llm = LLMFactory.create(provider="siliconflow")
agent = Agent(llm=llm, tools=[get_current_date], verbose=False)

# Run agent
response = agent.run("What is today's date?")

# Get registered functions
functions = agent.get_registered_functions()

# Access chat history
history = agent.chat_history
```

### Creating a Chatbot with Streaming

```python
from core.llm.chat import Chatbot
from core.llm.memory import LLMMemory

# Create chatbot with default memory
chatbot = Chatbot(system_prompt="You are a helpful assistant.")

# Create chatbot with custom memory
memory = LLMMemory.from_defaults(token_limit=5000)
chatbot = Chatbot(system_prompt="You are a helpful assistant.", memory=memory.get_memory())

# Stream response (async)
async for token in chatbot.astream_chat("Hello!"):
    print(token, end="", flush=True)

# Stream response (sync)
for token in chatbot.stream_chat("Hello!"):
    print(token, end="", flush=True)

# Non-streaming chat
response = chatbot.chat("Hello!")

# Reset conversation memory
chatbot.reset()

# Get chat history
history = chatbot.get_chat_history()
```

### Using RAG

```python
from core.llm.rag import Rag
from core.llm.model import LLMFactory
from core.llm.embedding import create_embedding_model

llm = LLMFactory.create()
embedding = create_embedding_model()
rag = Rag(llm=llm, embedding_model=embedding)

# Load documents (optional, can be done separately)
index = rag.load_document(document_path="./documents")

# Query documents
response = await rag.query(document_path="./documents", query="What is this about?")
```

### Using Advanced Memory

```python
from core.llm.memory import LLMMemory
from core.llm.model import LLMFactory
from core.llm.embedding import create_embedding_model
from llama_index.core.vector_stores import SimpleVectorStore

llm = LLMFactory.create()
embedding = create_embedding_model()

# Create memory with default settings (short-term only)
memory = LLMMemory.from_defaults(token_limit=3000)

# Create memory with static content
memory = LLMMemory.with_static_memory(
    static_content="User preferences: prefers concise answers",
    token_limit=3000
)

# Create memory with fact extraction
memory = LLMMemory.with_fact_extraction(llm=llm, token_limit=3000)

# Create memory with vector store
vector_store = SimpleVectorStore()
memory = LLMMemory.with_vector_memory(
    embed_model=embedding,
    vector_store=vector_store,
    token_limit=3000
)

# Create memory with all types (requires vector store)
vector_store = SimpleVectorStore()
memory = LLMMemory.with_all_memory_types(
    llm=llm,
    embed_model=embedding,
    vector_store=vector_store,
    static_content="User preferences: prefers concise answers",
    token_limit=3000
)

# Use with agent or chatbot
from core.llm.agent import Agent
agent = Agent(llm=llm, memory=memory.get_memory())

# Memory management
memory.reset()  # Clear all memory
messages = memory.get_all()  # Get all messages
memory.put(message)  # Add a message
retrieved = memory.get(**kwargs)  # Query messages
```

### Loading Prompts

```python
from core.llm.prompt import PromptLoader

prompt_loader = PromptLoader()
# Load prompt without variables
prompt = prompt_loader.load_prompt("template.liquid")

# Load prompt with variables
prompt = prompt_loader.load_prompt_with_context(
    "template.liquid",
    {"variable_name": "value"}
)
```

### Using LLM Factory

```python
from core.llm.model import LLMFactory, create_llm, LLMProvider

# Create with default provider (from LLM_PROVIDER env var, defaults to "azure_openai")
llm = LLMFactory.create()

# Create with specific provider
llm = LLMFactory.create(provider="azure_openai")
llm = LLMFactory.create(provider="siliconflow")

# Backward compatibility function
llm = create_llm()

# Get supported providers
providers = LLMFactory.get_supported_providers()  # Returns ["azure_openai", "siliconflow"]

# Register custom provider (advanced)
from llama_index.core.llms.function_calling import FunctionCallingLLM

def create_custom_llm() -> FunctionCallingLLM:
    # ... implementation ...
    pass

LLMFactory.register_provider("custom_provider", create_custom_llm)
```

## Dependencies

- **Pydantic v2**: For data validation and serialization
- **LlamaIndex 0.12.24**: Core framework for agents, RAG, embeddings, memory, and chat engines
- **Liquid**: For template rendering in prompt loader
- **python-dotenv**: For environment variable management

Refer to LlamaIndex documentation for Agents, RAG, Embeddings, Memory, Query Engines and Multi-Model for best practices.
